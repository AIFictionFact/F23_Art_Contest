{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Importing Libraries"
      ],
      "metadata": {
        "id": "5PUXRgNWjS7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OKHVWnGg4to"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data manipulation, image processing, and machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import PIL.Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Mounting Google Drive"
      ],
      "metadata": {
        "id": "z_m1RwIEjfxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive to access the dataset stored there\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "R294Dhqfjfdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Data Preparation"
      ],
      "metadata": {
        "id": "AOxTKCUsjwYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from CSV file\n",
        "df = pd.read_csv(\"drive/MyDrive/Group Project - Art Contest/Training Data/art_contest_dataset.csv\")\n",
        "# Group dataset by file name\n",
        "grouped = df.groupby('file_name')\n",
        "unique_fn = grouped.groups.keys()\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_fns, test_fns = train_test_split(list(unique_fn), test_size=0.2, random_state=42)\n",
        "train_data = df[df['file_name'].isin(train_fns)]\n",
        "test_data = df[df['file_name'].isin(test_fns)]"
      ],
      "metadata": {
        "id": "Qg286WvGjxiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: Saving Train and Test Sets"
      ],
      "metadata": {
        "id": "5EHhdCSDj1nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the split train and test data into CSV files\n",
        "train_data.to_csv(\"drive/MyDrive/Group Project - Art Contest/Training Data/train.csv\", index=False)\n",
        "test_data.to_csv(\"drive/MyDrive/Group Project - Art Contest/Training Data/test.csv\", index=False)"
      ],
      "metadata": {
        "id": "zL_ZXbWZj2-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5: Dataset Class Definition"
      ],
      "metadata": {
        "id": "p5CTxMGLj-NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for loading and transforming image data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with file paths and optional transformations.\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(data)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve an image and its label at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, label) where image is the transformed image tensor and label is a float tensor of the label.\n",
        "        \"\"\"\n",
        "        # Construct image file path\n",
        "        img_name = self.data.loc[idx, \"file_name\"].replace('/', '')\n",
        "        img_name = 'Copy of ' + img_name\n",
        "        img_path = os.path.join('/content/drive/MyDrive/Group Project - Art Contest/Training Data/all_training_art/', img_name)\n",
        "\n",
        "        # Load and convert the image to RGB\n",
        "        image = PIL.Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Map category labels to numerical values\n",
        "        label_map = {\"good_art\": 1, \"bad_art\": 0}\n",
        "        label = torch.tensor(label_map[self.data.loc[idx, \"category_id\"]])\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label.float()\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Resize images to fit model input size\n",
        "    transforms.ToTensor()          # Convert images to PyTorch tensors\n",
        "])"
      ],
      "metadata": {
        "id": "BH79fsw6kAFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6: Setting File Paths"
      ],
      "metadata": {
        "id": "jtctaWVokFhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for training and test data\n",
        "path = \"drive/MyDrive/Group Project - Art Contest/Training Data/\"\n",
        "train = os.path.join(path, 'train.csv')\n",
        "test = os.path.join(path, 'test.csv')"
      ],
      "metadata": {
        "id": "EM2MrVFukHI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7: Creating Datasets"
      ],
      "metadata": {
        "id": "-pd3rPgvkJy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize custom datasets for training and testing\n",
        "train_dataset = CustomDataset(train, transform=transform)\n",
        "test_dataset = CustomDataset(test, transform=transform)"
      ],
      "metadata": {
        "id": "DO2_uvE5kLAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8: DataLoader Initialization"
      ],
      "metadata": {
        "id": "GBATcYO4kNxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size for data loading\n",
        "batch_size = 32\n",
        "# Initialize DataLoader for batching operations\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "l-12S389kPJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9: Setting Up Device"
      ],
      "metadata": {
        "id": "pEMNtKZkkRYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability, else use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "IyxLIlGzkSzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10: Model Preparation"
      ],
      "metadata": {
        "id": "39tUuS7nkaaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "# Load a pre-trained ResNet-50 model\n",
        "model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
        "\n",
        "# Modify the model to fit the specific classification task\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 2)  # Output layer for binary classification\n",
        ")\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set up the optimizer for training the fully connected layers\n",
        "optimizer = Adam(model.fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5eh6KPB9kbeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 11: Freezing Layers"
      ],
      "metadata": {
        "id": "fFfg0c5LksoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the fully connected layers\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "    if name.startswith('fc'):\n",
        "        param.requires_grad = True"
      ],
      "metadata": {
        "id": "-ep5O4yZkuER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 12: Validation Function"
      ],
      "metadata": {
        "id": "2rQ5H3cIkwga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(dataloader, model):\n",
        "    \"\"\"\n",
        "    Validate the model on a given dataloader.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(dataloader.dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            correct_predictions += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_samples * 100\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8cAXUtqlkxt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 13: Training Loop"
      ],
      "metadata": {
        "id": "VEey3R2Kk2BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs for training\n",
        "num_epochs = 10\n",
        "best_accuracy = 0\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        labels = labels.type(torch.LongTensor)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {average_loss:.4f}\")\n",
        "\n",
        "    accuracy = valid(test_loader, model)\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model, 'model.pth')\n",
        "        print(f'Epoch {epoch + 1}: New best model with accuracy {best_accuracy:.2f}% saved.\\n\\n')"
      ],
      "metadata": {
        "id": "zzMsp8xIk3I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 14: Loading the Best Model"
      ],
      "metadata": {
        "id": "DHGr8_rwrrhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model from file\n",
        "PATH = '/content/model.pth'\n",
        "model = torch.load(PATH).to(device)"
      ],
      "metadata": {
        "id": "r_8Q12mwrsr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 15: Test Evaluation"
      ],
      "metadata": {
        "id": "_jsApHvfrvPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "model.eval()  # Set model to evaluation mode\n",
        "correct_predictions = 0\n",
        "total_samples = len(test_loader.dataset)\n",
        "predicted = []\n",
        "truth = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        truth.extend(labels.cpu().numpy())\n",
        "        correct_predictions += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
        "        predicted.extend(outputs.argmax(1).cpu().numpy())\n",
        "\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "GfPWJBpdrw1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 16: Confusion Matrix"
      ],
      "metadata": {
        "id": "ygOq9NXprzV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display the confusion matrix\n",
        "cm = confusion_matrix(truth, predicted)\n",
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)"
      ],
      "metadata": {
        "id": "8qGi68qir0fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 17: Displaying Confusion Matrix"
      ],
      "metadata": {
        "id": "oKo9IBqTr2rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-sNPLyGSr4EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 18: Installing Additional Packages"
      ],
      "metadata": {
        "id": "0QT3Qj4su6Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install multiple Python packages using pip in a single command\n",
        "!pip install tiktoken cohere openai requests"
      ],
      "metadata": {
        "id": "cE_ARFRcu73d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 19: Setting OpenAI API Key"
      ],
      "metadata": {
        "id": "tkDq350ju_nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OpenAI API key as an environment variable\n",
        "%env OPENAI_API_KEY=<your_actual_key_here>"
      ],
      "metadata": {
        "id": "MJoxYUW-vB9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 20: Image Generation and Ranking"
      ],
      "metadata": {
        "id": "8nhi4hIsviH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from ipywidgets import interact, widgets, VBox, HBox, HTML, Image as IPImage\n",
        "from IPython.display import display, Image, HTML\n",
        "from PIL import Image as PILImage\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import threading\n",
        "import time\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the classes for the art\n",
        "class_names = [\"good_art\", \"bad_art\"]\n",
        "\n",
        "# Initialize global variable to store the start time of image generation\n",
        "start_time = None\n",
        "\n",
        "# Initialize lists to store the generated images, prompts, and confidence scores\n",
        "generated_images = []\n",
        "generated_prompts = []\n",
        "generated_images_with_confidence = []\n",
        "\n",
        "def generate_ai_image(prompt, result_list, index, loading_bar):\n",
        "    \"\"\"\n",
        "    Generate an AI image based on the prompt and update the result list and loading bar.\n",
        "\n",
        "    Args:\n",
        "    - prompt (str): The prompt to generate the image from.\n",
        "    - result_list (list): The list to store generated images.\n",
        "    - index (int): The index in the result list to store the generated image.\n",
        "    - loading_bar (widgets.IntProgress): The loading bar widget to update progress.\n",
        "    \"\"\"\n",
        "    client = OpenAI()\n",
        "    response = client.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\",\n",
        "        quality=\"standard\",\n",
        "        n=1,\n",
        "    )\n",
        "    image_url = response.data[0].url\n",
        "    image_response = requests.get(image_url)\n",
        "    image_bytes = image_response.content\n",
        "    result_list[index] = image_bytes\n",
        "    loading_bar.value += 1\n",
        "\n",
        "def resize_images(images):\n",
        "    \"\"\"\n",
        "    Resize the input images to the required size for the model.\n",
        "\n",
        "    Args:\n",
        "    - images (list): List of PIL.Image objects.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Tensor containing the stacked resized image tensors.\n",
        "    \"\"\"\n",
        "    transformed_images = []\n",
        "    for img in images:\n",
        "        resized_img = img.resize((224, 224), PILImage.LANCZOS)\n",
        "        tensor_img = transforms.ToTensor()(resized_img)\n",
        "        transformed_images.append(tensor_img)\n",
        "    return torch.stack(transformed_images)\n",
        "\n",
        "def generate_and_display_images(prompt, model, class_names):\n",
        "    \"\"\"\n",
        "    Generate and display images based on the given prompt using the specified model.\n",
        "\n",
        "    Args:\n",
        "    - prompt (str): The prompt to generate images for.\n",
        "    - model (torch.nn.Module): The neural network model to rank images.\n",
        "    - class_names (list): The list of class names for classification.\n",
        "    \"\"\"\n",
        "    num_images = 5  # Set the desired number of images to generate\n",
        "    result_list = [None] * num_images\n",
        "    loading_bar = widgets.IntProgress(min=0, max=num_images, description='Generating:')\n",
        "    display(loading_bar)\n",
        "\n",
        "    global start_time\n",
        "    start_time = time.time()  # Record the start time for the generation process\n",
        "\n",
        "    # Start threads for image generation\n",
        "    threads = [threading.Thread(target=generate_ai_image, args=(prompt, result_list, i, loading_bar)) for i in range(num_images)]\n",
        "    for thread in threads:\n",
        "        thread.start()\n",
        "    for thread in threads:\n",
        "        thread.join()  # Wait for all threads to complete\n",
        "\n",
        "    # Convert the bytes to PIL images and perform resizing\n",
        "    pil_images = [PILImage.open(BytesIO(img)) for img in result_list if img is not None]\n",
        "    resized_images = resize_images(pil_images)\n",
        "\n",
        "    # Rank the images based on the neural network\n",
        "    sorted_images, ranked_info = rank_images(resized_images, model, class_names)\n",
        "\n",
        "    # Initialize lists for the image widgets and labels\n",
        "    ranked_widgets_images = []\n",
        "    confidence_labels = []\n",
        "    class_name_labels = []\n",
        "\n",
        "    # Create labels for the confidence and class names\n",
        "    for _, class_name, confidence in ranked_info:\n",
        "        confidence_labels.append(widgets.Label(f'Confidence: {confidence:.2%}'))\n",
        "        class_name_labels.append(widgets.Label(f'Class: {class_name}'))\n",
        "\n",
        "    # Create image widgets and associate them with confidence and class labels\n",
        "    for i, img in enumerate(sorted_images):\n",
        "        image_bytes = img.permute(1, 2, 0).cpu().numpy() * 255\n",
        "        image_bytes = image_bytes.astype('uint8')\n",
        "        image_pil = PILImage.fromarray(image_bytes)\n",
        "        image_bytes_io = BytesIO()\n",
        "        image_pil.save(image_bytes_io, format='PNG')\n",
        "        image_bytes_io.seek(0)\n",
        "        img_widget = widgets.Image(value=image_bytes_io.read(), format='png', width=224, height=224)\n",
        "        hbox = HBox([img_widget, VBox([confidence_labels[i], class_name_labels[i]])])\n",
        "        ranked_widgets_images.append(hbox)\n",
        "\n",
        "    # Create a tab widget to display ranked images with labels\n",
        "    tab = widgets.Tab()\n",
        "    tab.children = ranked_widgets_images\n",
        "    for i in range(len(ranked_widgets_images)):\n",
        "        tab.set_title(i, str(i))\n",
        "\n",
        "    # Display elapsed time and the tab widget\n",
        "    elapsed_time = time.time() - start_time\n",
        "    display(HTML(f'Total Elapsed Time: {elapsed_time:.2f} seconds'))\n",
        "    display(tab)\n",
        "\n",
        "def rank_images(images, model, class_names):\n",
        "    \"\"\"\n",
        "    Rank images using the provided model and return sorted images and ranking information.\n",
        "\n",
        "    Args:\n",
        "    - images (torch.Tensor): The images to rank.\n",
        "    - model (torch.nn.Module): The neural network model to use for ranking.\n",
        "    - class_names (list): The list of class names corresponding to the model's outputs.\n",
        "\n",
        "    Returns:\n",
        "    - list: Sorted images based on the confidence scores.\n",
        "    - list: Ranking information including image tensor, predicted class name, and confidence score.\n",
        "    \"\"\"\n",
        "    ranked_images = []\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for image in images:\n",
        "            # Process each image through the model and collect ranking info\n",
        "            image = image.unsqueeze(0).to(device)\n",
        "            outputs = model(image)\n",
        "            probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "            predicted_class_index = torch.argmax(probabilities).item()\n",
        "            confidence_score = probabilities[predicted_class_index].item()\n",
        "            predicted_class_name = class_names[predicted_class_index]\n",
        "            ranked_images.append((image.squeeze(0), predicted_class_name, confidence_score))\n",
        "    ranked_images.sort(key=lambda x: x[2], reverse=True)\n",
        "    return [img for img, _, _ in ranked_images], ranked_images\n",
        "\n",
        "# Widget setup for user interaction\n",
        "genre_dropdown = widgets.Dropdown(\n",
        "    options=[\"Abstract Expressionism\", \"Abstract\", \"Baroque\", \"Cubism\", \"Impressionism\", \"Minimalism\", \"Pop Art\", \"Realism\", \"Realism\", \"Renaissance\", \"Romanticism\", \"Surrealism\"],\n",
        "    description=\"Genre:\"\n",
        ")\n",
        "prompt_input = widgets.Text(description=\"Prompt:\", value=\"a portrait of a man\")\n",
        "generate_button = widgets.Button(description=\"Generate Images\")\n",
        "\n",
        "def on_generate_button_clicked(btn):\n",
        "    \"\"\"\n",
        "    Handle the click event on the 'Generate Images' button.\n",
        "\n",
        "    Args:\n",
        "    - btn (widgets.Button): The button instance that was clicked.\n",
        "    \"\"\"\n",
        "    prompt = prompt_input.value\n",
        "    selected_genre = genre_dropdown.value\n",
        "    generate_and_display_images(f\"{prompt} in the style of {selected_genre} art\", model, class_names)\n",
        "\n",
        "generate_button.on_click(on_generate_button_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(VBox([genre_dropdown, prompt_input, generate_button]))\n"
      ],
      "metadata": {
        "id": "H1YKTDlavkfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}